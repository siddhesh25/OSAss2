diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 887d3a7..6239358 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -2937,6 +2937,12 @@ config X86_DMA_REMAP
 config HAVE_GENERIC_GUP
        def_bool y

+menu "SJF schduler"
+config SCHED_SJF_POLICY
+       bool "enable SJF scheduler"
+       default y
+endmenu
+
 source "net/Kconfig"

 source "drivers/Kconfig"
diff --git a/fs/proc/Makefile b/fs/proc/Makefile
index ead487e..20bfad6 100644
--- a/fs/proc/Makefile
+++ b/fs/proc/Makefile
@@ -27,6 +27,7 @@ proc-y        += softirqs.o
 proc-y += namespaces.o
 proc-y += self.o
 proc-y += thread_self.o
+proc-$(CONFIG_SCHED_SJF_POLICY)        += proc_sjf.o
 proc-$(CONFIG_PROC_SYSCTL)     += proc_sysctl.o
 proc-$(CONFIG_NET)             += proc_net.o
 proc-$(CONFIG_PROC_KCORE)      += kcore.o
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43731fe..53a66e0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -498,6 +498,20 @@ struct sched_rt_entity {
 #endif
 } __randomize_layout;

+
+#ifdef CONFIG_SCHED_SJF_POLICY
+struct sched_sjf_entity {
+       unsigned int sjf_id;
+       unsigned long long rel_deadline;
+       unsigned long long rel_arrTime;
+       unsigned long long rel_burstTime;
+       unsigned long long absolute_deadline;
+
+       struct rb_node sjf_rb_node;
+       struct list_head sjf_list_node;
+};
+#endif /* CONFIG_SCHED_SJF_POLICY */
+
 struct sched_dl_entity {
        struct rb_node                  rb_node;

@@ -644,6 +658,10 @@ struct task_struct {
        const struct sched_class        *sched_class;
        struct sched_entity             se;
        struct sched_rt_entity          rt;
+#ifdef CONFIG_SCHED_SJF_POLICY
+       struct sched_sjf_entity         sjf;
+#endif
+
 #ifdef CONFIG_CGROUP_SCHED
        struct task_group               *sched_task_group;
 #endif
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index 22627f8..ac171e3 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -41,6 +41,8 @@
 #define SCHED_IDLE             5
 #define SCHED_DEADLINE         6

+#define SCHED_SJF              9
+
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000

diff --git a/include/uapi/linux/sched/types.h b/include/uapi/linux/sched/types.h
index 10fbb80..98f9903 100644
--- a/include/uapi/linux/sched/types.h
+++ b/include/uapi/linux/sched/types.h
@@ -70,6 +70,12 @@ struct sched_attr {
        __u64 sched_runtime;
        __u64 sched_deadline;
        __u64 sched_period;
+
+       /* SCHED_SJF */
+       //__u64 sjf_deadline;
+       __u64 sjf_arrTime;
+       __u64 sjf_burstTime;
+       __u64 sjf_id;
 };

 #endif /* _UAPI_LINUX_SCHED_TYPES_H */
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index d9a02b3..4d9b043 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -18,6 +18,7 @@ endif

 obj-y += core.o loadavg.o clock.o cputime.o
 obj-y += idle.o fair.o rt.o deadline.o
+obj-y += sched_sjf.o
 obj-y += wait.o wait_bit.o swait.o completion.o

 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o topology.o stop_task.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fe365c9..33782b3 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3416,6 +3416,12 @@ static void __sched notrace __schedule(bool preempt)
        struct rq *rq;
        int cpu;

+#ifdef  CONFIG_SCHED_SJF_POLICY
+/* Buffer for sjf to spritnf messages for event log */
+        char msg[SJF_MSG_SIZE];
+#endif
+
+
        cpu = smp_processor_id();
        rq = cpu_rq(cpu);
        prev = rq->curr;
@@ -3476,6 +3482,20 @@ static void __sched notrace __schedule(bool preempt)
        clear_tsk_need_resched(prev);
        clear_preempt_need_resched();

+#ifdef  CONFIG_SCHED_SJF_POLICY
+/* If either task involved in schedule() is a sjf task, log context_switch */
+        if(prev->policy==SCHED_SJF || next->policy==SCHED_SJF){
+               int prev_cid = prev->policy==SCHED_SJF?prev->sjf.sjf_id:-1;
+               int next_cid = next->policy==SCHED_SJF?next->sjf.sjf_id:-1;
+
+               snprintf(msg,SJF_MSG_SIZE,"prev->(cid%d:pid%d),next->(cid%d:pid%d)",
+                               prev_cid,prev->pid,next_cid,next->pid);
+                register_sjf_event(sched_clock(), msg, SJF_CONTEXT_SWITCH);
+
+
+        }
+#endif
+
        if (likely(prev != next)) {
                rq->nr_switches++;
                rq->curr = next;
@@ -4087,7 +4107,13 @@ static void __setscheduler_params(struct task_struct *p,

        p->policy = policy;

-       if (dl_policy(policy))
+       if (policy == SCHED_SJF) {
+               p->sjf.sjf_id = attr->sjf_id;
+               //p->sjf.rel_deadline = attr->sjf_deadline;
+               p->sjf.rel_arrTime = attr->sjf_arrTime;
+               p->sjf.rel_burstTime = attr->sjf_burstTime;
+               /* Don't initialize list or rb tree node */
+       } else if (dl_policy(policy))
                __setparam_dl(p, attr);
        else if (fair_policy(policy))
                p->static_prio = NICE_TO_PRIO(attr->sched_nice);
@@ -4116,7 +4142,9 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
        if (keep_boost)
                p->prio = rt_effective_prio(p, p->prio);

-       if (dl_prio(p->prio))
+       if (p->policy == SCHED_SJF) /* Normally scheduler determined by prio, sjf is exception*/
+               p->sched_class = &sjf_sched_class;
+       else if (dl_prio(p->prio))
                p->sched_class = &dl_sched_class;
        else if (rt_prio(p->prio))
                p->sched_class = &rt_sched_class;
@@ -6024,6 +6052,11 @@ void __init sched_init(void)
                init_cfs_rq(&rq->cfs);
                init_rt_rq(&rq->rt);
                init_dl_rq(&rq->dl);
+
+#ifdef CONFIG_SCHED_SJF_POLICY
+               init_sjf_rq(&rq->sjf);
+#endif /* CONFIG_SCHED_SJF_POLICY */
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
                root_task_group.shares = ROOT_TASK_GROUP_LOAD;
                INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
@@ -6112,6 +6145,10 @@ void __init sched_init(void)

        init_schedstats();

+#ifdef CONFIG_SCHED_SJF_POLICY
+       init_sjf_event_log();
+#endif /* CONFIG_SCHED_SJF_POLICY */
+
        scheduler_running = 1;
 }

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c7742dc..63cdc30 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -173,7 +173,7 @@ static inline int dl_policy(int policy)
 static inline bool valid_policy(int policy)
 {
        return idle_policy(policy) || fair_policy(policy) ||
-               rt_policy(policy) || dl_policy(policy);
+               rt_policy(policy) || dl_policy(policy) || policy == SCHED_SJF;
 }

 static inline int task_has_rt_policy(struct task_struct *p)
@@ -730,6 +730,16 @@ struct root_domain {
        unsigned long           max_cpu_capacity;
 };

+
+
+#ifdef CONFIG_SCHED_SJF_POLICY
+struct sjf_rq {
+       struct rb_root sjf_rb_root;
+       struct list_head sjf_list_head;
+       atomic_t nr_running;
+};
+#endif /* CONFIG_SCHED_SJF_POLICY */
+
 extern struct root_domain def_root_domain;
 extern struct mutex sched_domains_mutex;

@@ -785,6 +795,10 @@ struct rq {
        struct rt_rq            rt;
        struct dl_rq            dl;

+#ifdef CONFIG_SCHED_SJF_POLICY
+       struct sjf_rq           sjf;
+#endif /* CONFIG_SCHED_SJF_POLICY */
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
        /* list of leaf cfs_rq on this CPU: */
        struct list_head        leaf_cfs_rq_list;
@@ -1547,11 +1561,7 @@ static inline void set_curr_task(struct rq *rq, struct task_struct *curr)
        curr->sched_class->set_curr_task(rq);
 }

-#ifdef CONFIG_SMP
-#define sched_class_highest (&stop_sched_class)
-#else
-#define sched_class_highest (&dl_sched_class)
-#endif
+#define sched_class_highest (&sjf_sched_class)
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)

@@ -1561,6 +1571,10 @@ extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;

+#ifdef CONFIG_SCHED_SJF_POLICY
+extern const struct sched_class sjf_sched_class;
+#endif /* CONFIG_SCHED_SJF_POLICY */
+

 #ifdef CONFIG_SMP

@@ -2053,6 +2067,9 @@ print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);
 extern void init_dl_rq(struct dl_rq *dl_rq);
+#ifdef CONFIG_SCHED_SJF_POLICY
+extern void init_sjf_rq(struct sjf_rq *sjf_rq);
+#endif /* CONFIG_SCHED_SJF_POLICY */

 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
@@ -2194,3 +2211,36 @@ static inline unsigned long cpu_util_cfs(struct rq *rq)
        return util;
 }
 #endif
+
+
+
+#ifdef CONFIG_SCHED_SJF_POLICY
+
+/* Rolls its own logging system for events related to SJF */
+#define SJF_MSG_SIZE           400
+#define SJF_MAX_EVENT_LINES    10000
+
+#define SJF_ENQUEUE            1
+#define SJF_DEQUEUE            2
+#define        SJF_CONTEXT_SWITCH      3
+#define        SJF_MSG                 4
+
+#define SJF_EVENT_CODE(i) ("?EDSM?????"[i])
+
+struct sjf_event{
+       int action;
+       unsigned long long timestamp;
+       char msg[SJF_MSG_SIZE];
+};
+
+struct sjf_event_log{
+       struct sjf_event sjf_event[SJF_MAX_EVENT_LINES];
+       unsigned long lines;
+       unsigned long cursor;
+};
+
+void init_sjf_event_log(void);
+struct sjf_event_log * get_sjf_event_log(void);
+void register_sjf_event(unsigned long long t, char *m, int a);
+
+#endif